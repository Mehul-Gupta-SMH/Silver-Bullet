Project writeup
--------------------

### What is this about?

- Build a methodology to compare texts in different contexts which provide a score which can be used in further different contexts
  - Text 1 : Generated by LLM | Text 2 : Context used for generation | Score : Faithfulness
  - Text 1 : Generated by LLM | Text 2 : Generated by Another LLM | Score : How comparable is LLM 1 is to LLM 2 
  - Text 1 : Generated by LLM | Text 2 : Accepted Answers for RL | Score : How much aligned is the answer to accepted answer


### How to do this?
The technique is supposed to use multiple forms of scores between sentences to compare scores. 
- Text generated by LLM 1 : n sentences
- Text generated by LLM 2 : m sentences
- Create an nXm matrix of sentences and generate the combination scores:
  - Generate sentence embeddings and calculate cosine similarity (can use different models with specialization in specific domains) 
  - Find LCS (longest common sequence) between sentences
  - Find entity mapping between sentences (if same entity is being referred or not)

- This creates F features with nXm dimensions
- This info can be fed in via a CNN like architecture to train a classifier model to give out a single score (interpretation of score depends upon usage and is mentioned above)
- This trained model can be used as a teacher model and can be used to train a student model which only taken in embedding/attention scores of whole generated texts and give out a similar score

----------------------
### Anything like this?
- Closest counterpart of this is a cross-encoder or a re-ranker model which gives a cross-entropy loss between two texts
